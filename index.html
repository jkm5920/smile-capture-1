<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>스마일 캡처 앱</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>
    body {
      text-align: center;
      background: #222;
      color: #fff;
      font-family: sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
    }
    /* 비디오와 캔버스를 겹치기 위한 래퍼 */
    .video-container {
      position: relative;
      width: 320px;
      height: 240px;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      border-radius: 10px;
    }
    button {
      margin-top: 15px;
      padding: 10px 20px;
      border: none;
      border-radius: 8px;
      background: #0b84ff;
      color: white;
      font-size: 16px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <h2>스마일 캡처 앱</h2>
  <p>웃으면 자동으로 감지돼요!</p>
    <div class="video-container">
    <video id="video" width="320" height="240" autoplay muted></video>
    <canvas id="overlay" width="320" height="240"></canvas>
  </div>
  <button id="capture">사진 저장하기</button>

  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const context = overlay.getContext('2d');

    // 모델 로드
    Promise.all([
      faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/'),
      faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/')
    ]).then(startVideo);

    // 비디오 시작 및 카메라 권한 요청
    function startVideo() {
      navigator.mediaDevices.getUserMedia({ video: {} })
        .then(stream => {
          video.srcObject = stream;
        })
        .catch(err => {
          console.error("카메라 접근 실패:", err);
          alert("카메라 접근에 실패했습니다. 브라우저 설정(localhost)을 확인해주세요.");
        });
    }

    // 비디오가 재생될 때 얼굴 인식 시작
    video.addEventListener('play', () => {
      const displaySize = { width: video.width, height: video.height };
      // 캔버스 크기를 비디오에 맞게 설정
      faceapi.matchDimensions(overlay, displaySize);

      setInterval(async () => {
        // 얼굴 및 표정 감지
        const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
        
        // 감지 결과를 캔버스 크기에 맞게 리사이즈
        const resizedDetections = faceapi.resizeResults(detections, displaySize);
        
        // 캔버스를 지우고 새로 그리기
        context.clearRect(0, 0, overlay.width, overlay.height);
        faceapi.draw.drawDetections(overlay, resizedDetections);
        faceapi.draw.drawFaceExpressions(overlay, resizedDetections);

        // 행복한 표정이 0.9 (90%) 이상일 때 콘솔에 로그
        if (resizedDetections[0] && resizedDetections[0].expressions.happy > 0.9) {
          console.log("Smile detected! 😆");
        }
      }, 300); // 0.3초마다 감지
    });

    // 캡처 버튼 클릭 이벤트 (수정됨)
    document.getElementById('capture').addEventListener('click', () => {
      // 1. 임시 캔버스 생성
      const captureCanvas = document.createElement('canvas');
      captureCanvas.width = video.width;
      captureCanvas.height = video.height;
      const ctx = captureCanvas.getContext('2d');

      // 2. 임시 캔버스에 현재 비디오 화면을 그림
      ctx.drawImage(video, 0, 0, video.width, video.height);
      
      // 3. 그 위에 얼굴 인식 오버레이(상자, 표정)를 겹쳐서 그림
      ctx.drawImage(overlay, 0, 0, overlay.width, overlay.height);

      // 4. 완성된 이미지를 다운로드
      const link = document.createElement('a');
      link.href = captureCanvas.toDataURL('image/png'); // 임시 캔버스에서 이미지 데이터 추출
      link.download = 'smile_capture.png';
      link.click();
    });
  </script>
</body>
</html>

with open("index.html", "w", encoding="utf-8") as f:
    f.write(html_code)

print("index.html 파일 생성 완료!")
